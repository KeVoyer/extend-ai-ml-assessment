{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection in wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "from Model import MyNet\n",
    "import torch.optim as optim\n",
    "import torch.nn\n",
    "from pathlib import Path\n",
    "torch.manual_seed(17)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisqu'un défaut dans le bois n'est pas fonction de la grandeur on peut augmenter notre dataset en sélectionnant des parties aléatoires (RandomCrop). le choix de 224x224 est arbitraire. \n",
    "\n",
    "Dans le jeu de données toutes les images sont orientées dans la même direction (le grain du bois). Ce ne serait surement pas toujours le cas donc il vaut mieux ajouter une rotation dans les images ($\\pm 180 \\degree$). Cela permet d'éviter loverfitting sur les quelques défauts présents dans le jeu de données. On ne voudrait pas que le modele apprenne à repérer seulement certains types de fissure parce que celles-ci sont dans la bonne orientation\n",
    "\n",
    "Dans le problème actuel la couleur du bois est peu (pas) importante. On peut donc convertir nos images en noir et blanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pictures_size = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(pictures_size), \n",
    "    transforms.RandomRotation(180), \n",
    "     transforms.Grayscale(),\n",
    "     \n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, ), (0.5, )),\n",
    " ])\n",
    "\n",
    "dataset = datasets.ImageFolder(\"data\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=7, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour résoudre le problème de façon non supervisé j'ai cherché, et trouvé, l'article suivant : https://arxiv.org/pdf/2007.09990.pdf\n",
    "\n",
    "Ils ont rendu le code disponible sur github : https://github.com/kanezaki/pytorch-unsupervised-segmentation-tip\n",
    "\n",
    "J'ai donc utilisé leur modèle ainsi que la \"costom\" loss qui permet de pénaliser en fonction de la distance des pixel ainsi que de la variation entre la couleur (niveau de gris)\n",
    "\n",
    "On peut ajuster la sensibilité en modition les \"stepsize_con\" et \"stepsize_sim\" \n",
    "\n",
    "On pourrait aussi utiliser l'option \"scribble\" en entourant les anomalies (ca devient alors un probleme (semi) supervisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepoch = 100\n",
    "max_nb_class = 5\n",
    "\n",
    "visualize = False\n",
    "scribble =False\n",
    "\n",
    "stepsize_scr = 1\n",
    "stepsize_con = 1\n",
    "stepsize_sim = 3\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# MyNet(number_of_channel, maximum_number_of_classes, number_of_convolution + 2)\n",
    "model = MyNet(1, max_nb_class, 2)\n",
    "\n",
    "model.cuda() if use_cuda else model.cpu()\n",
    "model.train()\n",
    "# similarity loss definition\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# scribble loss definition\n",
    "loss_fn_scr = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# continuity loss definition\n",
    "loss_hpy = torch.nn.L1Loss(reduction=\"mean\")\n",
    "loss_hpz = torch.nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "HPy_target = torch.zeros(pictures_size - 1, pictures_size, max_nb_class)\n",
    "HPz_target = torch.zeros(pictures_size, pictures_size - 1, max_nb_class)\n",
    "\n",
    "if use_cuda:\n",
    "    HPy_target = HPy_target.cuda()\n",
    "    HPz_target = HPz_target.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5)\n",
    "label_colours = np.random.randint(255, size=(max_nb_class,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 100 | label num : 5 | loss : 3.7987093925476074\n",
      "1 / 100 | label num : 5 | loss : 3.0579702854156494\n",
      "2 / 100 | label num : 5 | loss : 1.9187875986099243\n",
      "3 / 100 | label num : 5 | loss : 2.6964402198791504\n",
      "4 / 100 | label num : 5 | loss : 2.036860466003418\n",
      "5 / 100 | label num : 3 | loss : 1.6885709762573242\n",
      "6 / 100 | label num : 5 | loss : 1.2485641241073608\n",
      "7 / 100 | label num : 4 | loss : 0.8898742198944092\n",
      "8 / 100 | label num : 4 | loss : 0.98454749584198\n",
      "9 / 100 | label num : 4 | loss : 0.9695890545845032\n",
      "10 / 100 | label num : 4 | loss : 0.8955908417701721\n",
      "11 / 100 | label num : 4 | loss : 0.34908005595207214\n",
      "12 / 100 | label num : 4 | loss : 0.4891376197338104\n",
      "13 / 100 | label num : 4 | loss : 0.30828577280044556\n",
      "14 / 100 | label num : 4 | loss : 0.3889220952987671\n",
      "15 / 100 | label num : 3 | loss : 0.3195057511329651\n",
      "16 / 100 | label num : 4 | loss : 0.3314145505428314\n",
      "17 / 100 | label num : 4 | loss : 0.5193002223968506\n",
      "18 / 100 | label num : 4 | loss : 0.557323694229126\n",
      "19 / 100 | label num : 4 | loss : 0.5272182822227478\n",
      "20 / 100 | label num : 4 | loss : 0.45370370149612427\n",
      "21 / 100 | label num : 2 | loss : 0.1457376778125763\n",
      "22 / 100 | label num : 3 | loss : 0.2723950147628784\n",
      "23 / 100 | label num : 4 | loss : 0.5506988763809204\n",
      "24 / 100 | label num : 3 | loss : 0.5418184995651245\n",
      "25 / 100 | label num : 4 | loss : 0.9207081198692322\n",
      "26 / 100 | label num : 4 | loss : 0.2711309492588043\n",
      "27 / 100 | label num : 4 | loss : 0.3092360496520996\n",
      "28 / 100 | label num : 4 | loss : 0.525873064994812\n",
      "29 / 100 | label num : 3 | loss : 0.44952982664108276\n",
      "30 / 100 | label num : 4 | loss : 0.2767205238342285\n",
      "31 / 100 | label num : 3 | loss : 0.19316521286964417\n",
      "32 / 100 | label num : 3 | loss : 0.24843907356262207\n",
      "33 / 100 | label num : 4 | loss : 0.5875350832939148\n",
      "34 / 100 | label num : 4 | loss : 0.36002117395401\n",
      "35 / 100 | label num : 4 | loss : 0.6530941724777222\n",
      "36 / 100 | label num : 4 | loss : 0.4208434522151947\n",
      "37 / 100 | label num : 4 | loss : 0.3968805968761444\n",
      "38 / 100 | label num : 4 | loss : 0.3237524926662445\n",
      "39 / 100 | label num : 3 | loss : 0.41933581233024597\n",
      "40 / 100 | label num : 4 | loss : 0.26761889457702637\n",
      "41 / 100 | label num : 3 | loss : 0.40283554792404175\n",
      "42 / 100 | label num : 3 | loss : 0.22688697278499603\n",
      "43 / 100 | label num : 4 | loss : 0.5508415699005127\n",
      "44 / 100 | label num : 4 | loss : 0.28894585371017456\n",
      "45 / 100 | label num : 3 | loss : 0.26245373487472534\n",
      "46 / 100 | label num : 4 | loss : 0.4667418599128723\n",
      "47 / 100 | label num : 4 | loss : 0.43237394094467163\n",
      "48 / 100 | label num : 4 | loss : 0.28303733468055725\n",
      "49 / 100 | label num : 4 | loss : 0.2581065893173218\n",
      "50 / 100 | label num : 4 | loss : 0.3498585820198059\n",
      "51 / 100 | label num : 4 | loss : 0.4731164872646332\n",
      "52 / 100 | label num : 4 | loss : 0.37293243408203125\n",
      "53 / 100 | label num : 4 | loss : 0.2574986219406128\n",
      "54 / 100 | label num : 3 | loss : 0.3037794530391693\n",
      "55 / 100 | label num : 4 | loss : 0.23738713562488556\n",
      "56 / 100 | label num : 4 | loss : 0.5027049779891968\n",
      "57 / 100 | label num : 4 | loss : 0.44072026014328003\n",
      "58 / 100 | label num : 4 | loss : 0.33129340410232544\n",
      "59 / 100 | label num : 4 | loss : 0.2846507132053375\n",
      "60 / 100 | label num : 4 | loss : 0.301313579082489\n",
      "61 / 100 | label num : 4 | loss : 0.49369898438453674\n",
      "62 / 100 | label num : 4 | loss : 0.5742169618606567\n",
      "63 / 100 | label num : 4 | loss : 0.4605404734611511\n",
      "64 / 100 | label num : 4 | loss : 0.567753791809082\n",
      "65 / 100 | label num : 4 | loss : 0.5780743360519409\n",
      "66 / 100 | label num : 4 | loss : 0.4299851655960083\n",
      "67 / 100 | label num : 4 | loss : 0.2557796835899353\n",
      "68 / 100 | label num : 3 | loss : 0.21291878819465637\n",
      "69 / 100 | label num : 4 | loss : 0.3206448554992676\n",
      "70 / 100 | label num : 3 | loss : 0.41502130031585693\n",
      "71 / 100 | label num : 4 | loss : 0.4149913787841797\n",
      "72 / 100 | label num : 4 | loss : 0.4249747097492218\n",
      "73 / 100 | label num : 4 | loss : 0.3938870429992676\n",
      "74 / 100 | label num : 4 | loss : 0.261160671710968\n",
      "75 / 100 | label num : 4 | loss : 0.27360814809799194\n",
      "76 / 100 | label num : 4 | loss : 0.33794546127319336\n",
      "77 / 100 | label num : 4 | loss : 0.2528446912765503\n",
      "78 / 100 | label num : 4 | loss : 0.5951065421104431\n",
      "79 / 100 | label num : 4 | loss : 0.23374518752098083\n",
      "80 / 100 | label num : 4 | loss : 0.26795583963394165\n",
      "81 / 100 | label num : 4 | loss : 0.47467106580734253\n",
      "82 / 100 | label num : 3 | loss : 0.19368140399456024\n",
      "83 / 100 | label num : 4 | loss : 0.46446579694747925\n",
      "84 / 100 | label num : 3 | loss : 0.33165204524993896\n",
      "85 / 100 | label num : 4 | loss : 0.5571527481079102\n",
      "86 / 100 | label num : 4 | loss : 0.4807932674884796\n",
      "87 / 100 | label num : 4 | loss : 0.411299467086792\n",
      "88 / 100 | label num : 3 | loss : 0.24854722619056702\n",
      "89 / 100 | label num : 3 | loss : 0.22812330722808838\n",
      "90 / 100 | label num : 3 | loss : 0.3225095272064209\n",
      "91 / 100 | label num : 4 | loss : 0.4930899143218994\n",
      "92 / 100 | label num : 4 | loss : 0.25078824162483215\n",
      "93 / 100 | label num : 4 | loss : 0.40912890434265137\n",
      "94 / 100 | label num : 4 | loss : 0.47440338134765625\n",
      "95 / 100 | label num : 4 | loss : 0.46288880705833435\n",
      "96 / 100 | label num : 4 | loss : 0.32957541942596436\n",
      "97 / 100 | label num : 4 | loss : 0.3479193449020386\n",
      "98 / 100 | label num : 4 | loss : 0.40205293893814087\n",
      "99 / 100 | label num : 3 | loss : 0.24433767795562744\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nepoch):\n",
    "    for data, _ in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)[0]\n",
    "\n",
    "        output = output.permute(1, 2, 0).contiguous().view(-1, max_nb_class)\n",
    "        outputHP = output.reshape((pictures_size, pictures_size, max_nb_class))\n",
    "\n",
    "        HPy = outputHP[1:, :, :] - outputHP[0:-1, :, :]\n",
    "        HPz = outputHP[:, 1:, :] - outputHP[:, 0:-1, :]\n",
    "        lhpy = loss_hpy(HPy, HPy_target)\n",
    "        lhpz = loss_hpz(HPz, HPz_target)\n",
    "\n",
    "        ignore, target = torch.max(output, 1)\n",
    "\n",
    "        im_target = target.data.cpu().numpy()\n",
    "        nLabels = len(np.unique(im_target))\n",
    "        if visualize:\n",
    "            im_target_rgb = np.array(\n",
    "                [label_colours[c % max_nb_class] for c in im_target]\n",
    "            )\n",
    "\n",
    "            im_target_rgb = im_target_rgb.reshape((pictures_size, pictures_size, 3)).astype(\n",
    "                np.uint8\n",
    "            )\n",
    "            cv2.imshow(\"output\", im_target_rgb)\n",
    "            cv2.waitKey(100)\n",
    "\n",
    "        # loss\n",
    "        if scribble:\n",
    "            loss = (\n",
    "                stepsize_sim * loss_fn(output[inds_sim], target[inds_sim])\n",
    "                + stepsize_scr\n",
    "                * loss_fn_scr(output[inds_scr], target_scr[inds_scr])\n",
    "                + stepsize_con * (lhpy + lhpz)\n",
    "            )\n",
    "        else:\n",
    "            loss = stepsize_sim * loss_fn(output, target) + stepsize_con * (\n",
    "                lhpy + lhpz\n",
    "            )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "    print(\n",
    "            f\"{epoch} / {nepoch} | label num : {nLabels} | loss : {loss.item()}\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mymodel.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNet(\n",
       "  (conv1): Conv2d(1, 5, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "  (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): ModuleList(\n",
       "    (0): Conv2d(5, 5, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "  )\n",
       "  (bn2): ModuleList(\n",
       "    (0): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): Conv2d(5, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyNet(1, max_nb_class, 2)\n",
    "model.load_state_dict(torch.load('mymodel.save'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr = transforms.Compose([\n",
    "     transforms.Grayscale(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, ), (0.5, )),\n",
    "     ]\n",
    " )\n",
    "\n",
    "paths = glob.glob('data/train/*.jpg')\n",
    "\n",
    "for path in paths:\n",
    "    img = Image.open(path)\n",
    "    data = tr(img).unsqueeze(0)\n",
    "\n",
    "    output = model(data)[0]\n",
    "    output = output.permute(1, 2, 0).contiguous().view(-1, max_nb_class)\n",
    "    ignore, target = torch.max(output, 1)\n",
    "   \n",
    "    im_target = target.data.cpu().numpy()\n",
    "    im_target_rgb = np.array([label_colours[c % max_nb_class] for c in im_target])\n",
    "    im_target_rgb = im_target_rgb.reshape((img.size[1], img.size[0], 3)).astype(np.uint8)\n",
    "\n",
    "    \n",
    "    Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(f\"results/{path.rsplit('/',1)[1]}\", im_target_rgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After you finish the challenge, propose how you would improve the model in the near future\n",
    "1. I would create a log for every experiment\n",
    "2. I would try different model architecture (Unet)\n",
    "3. I would modify the loss function, number of classes at the begining, learning rate, change optimizer to see if we are stuck in a local minima. \n",
    "4. I would also augment the dataset permanently instead of cropping and \"rotating\" on the fly so we can see what the model had been seeing.\n",
    "5. Talking about cropping and rotation, I would search for an implementation of a tansformation that rotate and crop the picture inside of it so we can get rid of border effect\n",
    "6. I would also try different shape for the pictures \n",
    "7. I would try to change de aspect ratio that way we can get new kind of anomalies\n",
    "\n",
    "# how you would transfer learnings to other types of surfaces (i.e. not wood).\n",
    "\n",
    "I think it's problem dependant but if it wasnt wodd maybe color would be important and maybe the size of kernel in the convolution would need to be larger if anomalies are bigger"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb86d2497654d5e221ed42f8a37cadbea5e752644e70ad57e49e6b60205c9146"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
